{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "035c6209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "from sklearn import metrics\n",
    "import multiprocessing as mp\n",
    "import os.path as osp\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "from PIL import Image, ImageFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3af0392b-60e3-4b89-b111-bf90944fa208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import shutil\n",
    "from multiprocessing import Pool\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from byol_pytorch import BYOL\n",
    "from torchvision import models\n",
    "import torchvision.transforms as T\n",
    "import random\n",
    "from torch import nn\n",
    "import time\n",
    "import scanpy as sc\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5464553b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b86cc0b-6bb2-4b66-8999-cb20e6dbc0a6",
   "metadata": {},
   "source": [
    "<h4>Input</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d660fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6316/2328745661.py:3: FutureWarning: Use `squidpy.read.visium` instead.\n",
      "  adata = sc.read_visium(file_fold, count_file='filtered_feature_bc_matrix.h5', load_images=True)\n",
      "/home/isaac/dev/sfu/cmpt415/CLGraph/CLGraph/clgraph/lib/python3.12/site-packages/anndata/_core/anndata.py:1776: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n",
      "/home/isaac/dev/sfu/cmpt415/CLGraph/CLGraph/clgraph/lib/python3.12/site-packages/anndata/_core/anndata.py:1776: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 3673 × 33538\n",
       "    obs: 'in_tissue', 'array_row', 'array_col'\n",
       "    var: 'gene_ids', 'feature_types', 'genome'\n",
       "    uns: 'spatial'\n",
       "    obsm: 'spatial'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = '151674'\n",
    "file_fold = '/home/isaac/dev/sfu/cmpt415/DeepST/data/DLPFC/' + str(dataset)\n",
    "adata = sc.read_visium(file_fold, count_file='filtered_feature_bc_matrix.h5', load_images=True)\n",
    "adata.var_names_make_unique()\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc4e8ae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13332, 13332, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_path = \"/home/isaac/dev/sfu/cmpt415/hest_data/wsis/MISC3.tif\"\n",
    "im = cv2.imread(im_path,1)   \n",
    "im.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac70f21-a9b0-44ee-9034-47d3e66edb60",
   "metadata": {},
   "source": [
    "<h4>Clipping</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c38fa597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'clip_image' created successfully\n"
     ]
    }
   ],
   "source": [
    "# Create directory for clipped images\n",
    "clip_image_path = os.path.join(file_fold, 'clip_image')\n",
    "\n",
    "try:\n",
    "    os.makedirs(clip_image_path)\n",
    "    print(\"Folder 'clip_image' created successfully\")\n",
    "except FileExistsError:\n",
    "    shutil.rmtree(clip_image_path)\n",
    "    os.makedirs(clip_image_path)\n",
    "    print(\"Folder 'clip_image' already exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7750aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3673/3673 [00:43<00:00, 85.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# Process and save patches\n",
    "# patch_size = 512\n",
    "\n",
    "# patch_size = 256\n",
    "\n",
    "d = 144\n",
    "patch_size = int(3.5*d)\n",
    "\n",
    "# patch_size = 140\n",
    "patches = []\n",
    "for i, coord in tqdm(enumerate(adata.obsm['spatial']), total=len(adata.obsm['spatial'])):\n",
    "    # print(coord)\n",
    "    # Calculate patch coordinates\n",
    "    left = int(coord[0] - patch_size / 2)\n",
    "    top = int(coord[1] - patch_size / 2)\n",
    "    right = left + patch_size\n",
    "    bottom = top + patch_size\n",
    "\n",
    "    # Extract patch\n",
    "    patch = im[top:bottom, left:right]\n",
    "\n",
    "    # Resize patch to 512x512, using INTER_LINEAR for both upsizing and downsizing\n",
    "    if patch_size != 512:\n",
    "        resized_patch = cv2.resize(patch, (512, 512), interpolation=cv2.INTER_LINEAR)\n",
    "    else:\n",
    "        resized_patch = patch\n",
    "    # Save resized patch\n",
    "    cv2.imwrite(os.path.join(clip_image_path, f'{i}.png'), resized_patch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760c55b9-b790-4267-9236-41f5793851a1",
   "metadata": {},
   "source": [
    "<h4>filtering</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "289ec4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'clip_image_filter' already exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@707.705] global loadsave.cpp:1063 imwrite_ Unsupported depth image for selected encoder is fallbacked to CV_8U.\n",
      "[ WARN:0@707.705] global loadsave.cpp:1063 imwrite_ Unsupported depth image for selected encoder is fallbacked to CV_8U.\n",
      "[ WARN:0@707.720] global loadsave.cpp:1063 imwrite_ Unsupported depth image for selected encoder is fallbacked to CV_8U.\n",
      "[ WARN:0@707.720] global loadsave.cpp:1063 imwrite_ Unsupported depth image for selected encoder is fallbacked to CV_8U.\n",
      "[ WARN:0@707.720] global loadsave.cpp:1063 imwrite_ Unsupported depth image for selected encoder is fallbacked to CV_8U.\n",
      "[ WARN:0@707.723] global loadsave.cpp:1063 imwrite_ Unsupported depth image for selected encoder is fallbacked to CV_8U.\n",
      "[ WARN:0@707.728] global loadsave.cpp:1063 imwrite_ Unsupported depth image for selected encoder is fallbacked to CV_8U.\n",
      "[ WARN:0@707.728] global loadsave.cpp:1063 imwrite_ Unsupported depth image for selected encoder is fallbacked to CV_8U.\n",
      "[ WARN:0@707.732] global loadsave.cpp:1063 imwrite_ Unsupported depth image for selected encoder is fallbacked to CV_8U.\n",
      "[ WARN:0@707.733] global loadsave.cpp:1063 imwrite_ Unsupported depth image for selected encoder is fallbacked to CV_8U.\n",
      "[ WARN:0@707.733] global loadsave.cpp:1063 imwrite_ Unsupported depth image for selected encoder is fallbacked to CV_8U.\n",
      "[ WARN:0@707.734] global loadsave.cpp:1063 imwrite_ Unsupported depth image for selected encoder is fallbacked to CV_8U.\n",
      "[ WARN:0@707.734] global loadsave.cpp:1063 imwrite_ Unsupported depth image for selected encoder is fallbacked to CV_8U.\n",
      "[ WARN:0@707.734] global loadsave.cpp:1063 imwrite_ Unsupported depth image for selected encoder is fallbacked to CV_8U.\n"
     ]
    }
   ],
   "source": [
    "def process_image(filename, path, output_path, GaussianBlur, lower, upper):\n",
    "    image_path = os.path.join(path, filename)\n",
    "    image = cv2.imread(image_path, 0)\n",
    "    original_height, original_width = image.shape[:2]\n",
    "\n",
    "    resize_needed = (original_height != 512 or original_width != 512)\n",
    "    if resize_needed:\n",
    "        image = cv2.resize(image, (512, 512))\n",
    "\n",
    "    if GaussianBlur:\n",
    "        image = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "    f = np.fft.fft2(image)\n",
    "    fshift = np.fft.fftshift(f)\n",
    "    image_shape = image.shape\n",
    "    custom_mask = create_custom_mask(image_shape, lower, lower, upper, upper)\n",
    "    fshift_masked = fshift * custom_mask\n",
    "    f_ishift = np.fft.ifftshift(fshift_masked)\n",
    "    image_filtered = np.fft.ifft2(f_ishift)\n",
    "    image_filtered = np.abs(image_filtered)\n",
    "    if GaussianBlur:\n",
    "        image_filtered = cv2.GaussianBlur(image_filtered, (15, 15), 0)\n",
    "    image_filtered_rgb = cv2.cvtColor(np.float32(image_filtered), cv2.COLOR_GRAY2RGB)\n",
    "    mae_patch = cv2.resize(image_filtered_rgb, (224, 224), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    if resize_needed:\n",
    "        image_filtered_rgb = cv2.resize(image_filtered_rgb, (original_width, original_height))\n",
    "    cv2.imwrite(os.path.join(output_path, filename), image_filtered_rgb)\n",
    "    return mae_patch\n",
    "\n",
    "def create_custom_mask(image_shape, x1, y1, x2, y2):\n",
    "    rows, cols = image_shape\n",
    "    mask = np.zeros((rows, cols), np.uint8)\n",
    "    mask[y1:y2, x1:x2] = 1\n",
    "    return mask\n",
    "\n",
    "\n",
    "path = os.path.join(file_fold ,'clip_image')\n",
    "output_path =  os.path.join(file_fold,'clip_image_filter')\n",
    "GaussianBlur = True\n",
    "upper = 275\n",
    "lower = 245\n",
    "\n",
    "try:\n",
    "    os.makedirs(output_path)\n",
    "    print(\"Folder 'clip_image_filter' created successfully\")\n",
    "except FileExistsError:\n",
    "    shutil.rmtree(output_path)\n",
    "    os.makedirs(output_path)\n",
    "    print(\"Folder 'clip_image_filter' already exist\")\n",
    "\n",
    "png_files = [name for name in os.listdir(path) if name.endswith('.png')]\n",
    "args = [(filename, path, output_path, GaussianBlur, lower, upper) for filename in png_files]\n",
    "\n",
    "with Pool(processes=os.cpu_count() - 2) as pool:\n",
    "    patches = pool.starmap(process_image, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912eacfc-ec38-4613-ace3-1eb4139a4c26",
   "metadata": {},
   "source": [
    "<h4>BYOL</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f9a3409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isaac/dev/sfu/cmpt415/CLGraph/CLGraph/clgraph/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/isaac/dev/sfu/cmpt415/CLGraph/CLGraph/clgraph/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /home/isaac/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 97.8M/97.8M [00:00<00:00, 112MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training!\n",
      "Epoch [1/3], Loss: 0.1949, Time: 60.40 seconds\n",
      "Epoch [2/3], Loss: 0.1238, Time: 59.00 seconds\n",
      "Epoch [3/3], Loss: 0.3096, Time: 59.20 seconds\n",
      "start eval!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation Progress: 100%|█████████████████| 3673/3673 [00:32<00:00, 112.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3673, 2048)\n"
     ]
    }
   ],
   "source": [
    "def process_images(epoch_num):\n",
    "    path = os.path.join(file_fold,'clip_image_filter')\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(42)\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8' \n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "    class CustomDataset(Dataset):\n",
    "        def __init__(self, root_dir, transform=None):\n",
    "            self.root_dir = root_dir\n",
    "            self.transform = transform\n",
    "            self.image_list = sorted([x for x in os.listdir(root_dir) if x.endswith('.png')], key=lambda x: int(x.split('.')[0]))\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.image_list)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            img_name = os.path.join(self.root_dir, self.image_list[idx])\n",
    "            img = Image.open(img_name).convert('RGB')\n",
    "\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "\n",
    "            return img\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    dataset = CustomDataset(path, transform=transform)\n",
    "    data_loader = DataLoader(dataset, batch_size=21, shuffle=True, num_workers=1) \n",
    "\n",
    "    class RandomApply(nn.Module):\n",
    "        def __init__(self, fn, p):\n",
    "            super().__init__()\n",
    "            self.fn = fn\n",
    "            self.p = p\n",
    "\n",
    "        def forward(self, x):\n",
    "            if random.random() > self.p:\n",
    "                return x\n",
    "            return self.fn(x)\n",
    "\n",
    "    DEFAULT_AUG = torch.nn.Sequential(\n",
    "        RandomApply(T.ColorJitter(0.8, 0.8, 0.8, 0.2), p=0.3),\n",
    "        T.RandomGrayscale(p=0.2),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.RandomVerticalFlip(),\n",
    "        RandomApply(T.GaussianBlur((3, 3), (1.0, 2.0)), p=0.2),\n",
    "        T.RandomRotation(degrees=(0, 360)),\n",
    "        T.RandomResizedCrop((256, 256)),\n",
    "        T.Normalize(mean=torch.tensor([0.485, 0.456, 0.406]), std=torch.tensor([0.229, 0.224, 0.225])),\n",
    "    )\n",
    "\n",
    "    learner = BYOL(\n",
    "            models.resnet50(pretrained=True),\n",
    "            image_size=256,\n",
    "            hidden_layer='avgpool',\n",
    "            augment_fn=DEFAULT_AUG\n",
    "        )\n",
    "    if torch.cuda.is_available():\n",
    "        learner = learner.cuda()\n",
    "\n",
    "    opt = torch.optim.Adam(learner.parameters(), lr=3e-4)\n",
    "\n",
    "    print('start training!')\n",
    "    for epoch in range(epoch_num):\n",
    "        start_time = time.time()\n",
    "        for images in data_loader:\n",
    "            images = images.cuda() if torch.cuda.is_available() else images\n",
    "            loss = learner(images)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            learner.update_moving_average()\n",
    "        end_time = time.time()\n",
    "        print(f'Epoch [{epoch + 1}/{epoch_num}], Loss: {loss.item():.4f}, Time: {end_time - start_time:.2f} seconds')\n",
    "\n",
    "    torch.save(learner.state_dict(), 'learner.pth')\n",
    "\n",
    "    learner.eval()\n",
    "    embeddings = []\n",
    "    print('start eval!')\n",
    "    for i in tqdm(range(len(dataset)), desc='Evaluation Progress'):\n",
    "        img = dataset[i]\n",
    "        img = img.cuda() if torch.cuda.is_available() else img\n",
    "        with torch.no_grad():\n",
    "            _, embedding = learner(img.unsqueeze(0), return_embedding=True)\n",
    "            embeddings.append(embedding.cpu().numpy())\n",
    "\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    np.save(os.path.join(file_fold,'embeddings.npy'), embeddings)\n",
    "    print(embeddings.shape)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "epoch_num=3\n",
    "embeddings = process_images(epoch_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fdb61fc3-1077-4ea6-91b8-a334864db8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(osp.join(file_fold,\"embeddings.npy\"),embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
